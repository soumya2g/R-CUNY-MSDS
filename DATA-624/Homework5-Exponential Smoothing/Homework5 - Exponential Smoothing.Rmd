---
title: "DATA 624 - Homework5 - Exponential Smoothing"
author: "Soumya Ghosh"
date: "September 20, 2020"
always_allow_html: yes
output:
  html_document:
    df_print: kable
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(TSstudio)
library(RColorBrewer)
library(GGally)
library(fpp2)
library(seasonal)
library(grid)
library(gridExtra)
library(forecast)
```

## Forecasting: Principles & Practice

### Section 7.8 Exercise 1

#### DataSet: pigs

**Description:** - The number of pigs slaughtered in Victoria each month.

```{r warning=FALSE, message=FALSE}
pigs
```

#### Timeseries data plot: pigs

```{r warning=FALSE, message=FALSE}
autoplot(pigs,ylab="No. of Pigs Slaughtered",xlab="Year") + ggtitle("Pigs Slaughtered Trend in Victoria, Australia")
```

a) Use the ses() function in R to find the optimal values of  $\alpha$ and ${ l }_{ 0 }$, and generate forecasts for the next four months.

```{r warning=FALSE, message=FALSE}
# Estimate parameters & generate 4 months forecast (h=4)
fit <- ses(pigs, h=4)
summary(fit)

```

Based on the above output of **ses()** -

Optimal value of **smoothing parameter $\alpha$ is 0.2971** and **${ l }_{ 0 }$ is 77260.0561.** 


b) Compute a 95% prediction interval for the first forecast using $\overset { \^  }{ y } \pm 1.96s$ where s is the standard deviation of the residuals. Compare your interval with the interval produced by R.

#### 95% Confidence Interval

```{r warning=FALSE, message=FALSE}
# Standard Deviation of the Residuals
sd(residuals(fit))

cat("Floor Value of the 95% Prediction interval for the point forecast:",98816.41 - 1.96*sd(residuals(fit)))
cat("Ceiling Value of the 95% Prediction interval for the point forecast:",98816.41 + 1.96*sd(residuals(fit)))

```

Comparing the Prediction interval values calculayed above and those generated by R, lower value of the calculated interval is greater and ceiling value is lesser. So the calculated prediction interval appear to be smaller than the range derived by R.


### Section 7.8 Exercise 5

#### DataSet: books

**Description:** - Data set books contains the daily sales of paperback and hardcover books at the same store. 

```{r warning=FALSE, message=FALSE}
books
```


The task is to forecast the next four days' sales for paperback and hardcover books.

a) Plot the series and discuss the main features of the data.

#### Timeseries data plot: books

```{r warning=FALSE, message=FALSE}
autoplot(books,ylab="Books Sales",xlab="Days") + ggtitle("Trend of Books Sales")
```

#### Observations

  - Overall there are more sales in Hardcover books than the Paperback books. 
  - Both types of books have a pattern resembling a positive trend. 
  - The peaks and troughs happens at irregular intervals therefore there is not any reason to conclude there is seasonality present.


b) Use the ses() function to forecast each series, and plot the forecasts.

#### Books Sales Forecast using SES: Paperback
```{r warning=FALSE, message=FALSE}
# Forecast for 4 days
ses_p_fit <- ses(books[,"Paperback"], h=4)
summary(ses_p_fit)

# Forecast Plot
autoplot(ses_p_fit) +
  autolayer(fitted(ses_p_fit), series='Fitted') +
  ggtitle('SES Fit and Forecast of Paperback Sales') +
  xlab('Day') +
  ylab('Books Sales')
```

#### Books Sales Forecast using SES: Hardcover

```{r warning=FALSE, message=FALSE}
# Forecast for 4 days
ses_h_fit <- ses(books[,"Hardcover"], h=4)
summary(ses_h_fit)

# Forecast Plot
autoplot(ses_h_fit) +
  autolayer(fitted(ses_h_fit), series='Fitted') +
  ggtitle('SES Fit and Forecast of Hardcover Sales') +
  xlab('Day') +
  ylab('Books Sales')
```

c) Compute the RMSE values for the training data in each case.

#### RMSE Values of Training Data Set

```{r warning=FALSE, message=FALSE}
cateory <- c("Paperback","Hardcover")

ses_rmse <- c(accuracy(ses_p_fit)[,"RMSE"],accuracy(ses_h_fit)[,"RMSE"]) 

cat("RMSE Value for the Paperback data set:",accuracy(ses_p_fit)[,"RMSE"])

cat("RMSE Value for the Hardcover data set:",accuracy(ses_h_fit)[,"RMSE"])
```

### Section 7.8 Exercise 6
We will continue with the daily sales of paperback and hardcover books in data set books.

a) Apply Holt's linear method to the paperback and hardback series and compute four-day forecasts in each case.

#### Holt's Linear Trend method for Forecast: Paperback

```{r  warning=FALSE, message=FALSE}
holt_p_fit <- holt(books[,"Paperback"], h=4)

summary(holt_p_fit)
autoplot(books[,"Paperback"]) +
  autolayer(fitted(holt_p_fit), series='Fitted') +
  autolayer(holt_p_fit, series="Holt's method", PI=TRUE) +
  ggtitle("4 Days Paperback Book Sales Forecasts from Holt's method") + xlab("Days") +
  ylab("Book Sales") +
  guides(colour=guide_legend(title="Forecast"))
```

#### Holt's Linear Trend method for Forecast: Hardcover

```{r  warning=FALSE, message=FALSE}
holt_h_fit <- holt(books[,"Hardcover"], h=4)

summary(holt_h_fit)
autoplot(books[,"Hardcover"]) +
  autolayer(fitted(holt_h_fit), series='Fitted') +
  autolayer(holt_h_fit, series="Holt's method", PI=TRUE) +
  ggtitle("4 Days Hardcover Book Sales Forecasts from Holt's method") + xlab("Days") +
  ylab("Book Sales") +
  guides(colour=guide_legend(title="Forecast"))
```


b) Compare the RMSE measures of Holt's method for the two series to those of simple exponential smoothing in the previous question. (Remember that Holt's method is using one more parameter than SES.) Discuss the merits of the two forecasting methods for these data sets.

#### Comparison of Forecasting Methods: SES Vs. Holt's

```{r warning=FALSE, message=FALSE}
holts_rmse <- c(accuracy(holt_p_fit)[,"RMSE"],accuracy(holt_h_fit)[,"RMSE"]) 

cat("RMSE Value for the Paperback data set:",accuracy(holt_p_fit)[,"RMSE"])
cat("RMSE Value for the Hardcover data set:",accuracy(holt_h_fit)[,"RMSE"])


RMSECompTable <- cbind(cateory,ses_rmse,holts_rmse) 
colnames(RMSECompTable) <- c("BookCategory","RMSE-SES","RMSE-Holt's")

RMSECompTable %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="150px")

```

After carefully reviewing the above plots and the RMSE comaprison table -

  - Holt's Linear Trend method generated smaller RMSE than the SES model for both books categories
  - SES method is better suited for data sets without any visible trend component. But for Book Sales data, inspte of having plenty of peaks and valleys for both the categories, there is a visible positive trend in data. Holt's method accounts for the Trend component of a time series through an additional smoothing parameter ${ \beta  }^{ * }$ compared to SES method.  
  - So based on lower RMSE and analyzing the plots associated with Holt's method, I think Holt's method is better suited in forecasting the books data sets.



c) Compare the forecasts for the two series using both methods. Which do you think is best?

#### Forecast comparision: SES Vs. Holt's:

 - SES method generated ***"flat"*** forecasts completely ignoring the strong positive trends in both the books categories.
 - Holt's method captured the positive trend as discussed in the previous section and applied the calculated slope in generating forecasts for 4 days of sales.
 - I think Holt's method generated forecasts are more reliable since it captured the inherent trend component in the training data. 

d) Calculate a 95% prediction interval for the first forecast for each series, using the RMSE values and assuming normal errors. Compare your intervals with those produced using ses and holt.

#### 95% Prediction Interval Comparison based on RMSE

Assuming normally distributed errors, we can construct the 95% prediction interval using $\overset { \^  }{ y } \pm 1.96*RMSE$ equation -

```{r warning=FALSE, message=FALSE}
# First Point Forecasts
ses_p_fit_1 <- ses_p_fit$mean[1]
ses_h_fit_1 <- ses_h_fit$mean[1]
holt_p_fit_1 <- holt_p_fit$mean[1]
holt_h_fit_1 <- holt_h_fit$mean[1]


#### Derived 95% Prediction Interval Based on RMSE

## Paperback Books - SES - R Generated
ses_lower_p_r <- ses_p_fit$lower[1,2]
ses_upper_p_r <- ses_p_fit$upper[1,2]

## Paperback Books - SES - Calculated 
ses_lower_p <- ses_p_fit$mean[1] - 1.96 * accuracy(ses_p_fit)[,"RMSE"]
ses_upper_p <- ses_p_fit$mean[1] + 1.96 * accuracy(ses_p_fit)[,"RMSE"]


## Hardcover Books - SES - R Generated
ses_lower_h_r <- ses_h_fit$lower[1,2]
ses_upper_h_r <- ses_h_fit$upper[1,2]

## Hardcover Books - SES
ses_lower_h <- ses_h_fit$mean[1] - 1.96 * accuracy(ses_h_fit)[,"RMSE"]
ses_upper_h <- ses_h_fit$mean[1] + 1.96 * accuracy(ses_h_fit)[,"RMSE"]


## Paperback Books - Holt's - R Generated
holt_lower_p_r <- holt_p_fit$lower[1,2]
holt_upper_p_r <- holt_p_fit$upper[1,2]

## Paperback Books - Holt
holt_lower_p <- holt_p_fit$mean[1] - 1.96 * accuracy(holt_p_fit)[,"RMSE"]
holt_upper_p <- holt_p_fit$mean[1] + 1.96 * accuracy(holt_p_fit)[,"RMSE"]

## Hardcover Books - Holt's - R Generated
holt_lower_h_r <- holt_h_fit$lower[1,2]
holt_upper_h_r <- holt_h_fit$upper[1,2]

## Hardcover Books - Holt
holt_lower_h <- holt_h_fit$mean[1] - 1.96 * accuracy(holt_h_fit)[,"RMSE"]
holt_upper_h <- holt_h_fit$mean[1] + 1.96 * accuracy(holt_h_fit)[,"RMSE"]

Col1 <- c(ses_p_fit_1, ses_lower_p_r, ses_upper_p_r)
Col2 <- c(ses_p_fit_1, ses_lower_p, ses_upper_p)
Col3 <- c(holt_p_fit_1, holt_lower_p_r, holt_upper_p_r)
Col4 <- c(holt_p_fit_1, holt_lower_p, holt_upper_p)

Col5 <- c(ses_h_fit_1, ses_lower_h_r, ses_upper_h_r)
Col6 <- c(ses_h_fit_1, ses_lower_h, ses_upper_h)
Col7 <- c(holt_h_fit_1, holt_lower_h_r, holt_upper_h_r)
Col8 <- c(holt_h_fit_1, holt_lower_h, holt_upper_h)



df <- data.frame(Col1,Col2,Col3,Col4,Col5,Col6,Col7,Col8)
df[4,] <- df[3,] - df[2,]

colnames(df) <- c('R - SES', 'Calculated - SES', 'R - Holt','Calculated - Holt', 'R - SES', 'Calculated - SES', 'R - Holt','Calculated - Holt')
row.names(df) <- c('Point Forecast', '95% - Lower', '95% - Upper', 'Interval Range')

kable(df) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  add_header_above(c(' ', 'Paperback Forecast' = 4, 'Hardcover Forecost' = 4)) %>% scroll_box(width="100%",height="300px")

```



### Section 7.8 Exercise 7
For this exercise use data set eggs, the price of a dozen eggs in the United States from 1900-1993. Experiment with the various options in the holt() function to see how much the forecasts change with damped trend, or with a Box-Cox transformation. Try to develop an intuition of what each argument is doing to the forecasts.

[Hint: use h=100 when calling holt() so you can clearly see the differences between the various options when plotting the forecasts.]

Which model gives the best RMSE?

#### DataSet: eggs

**Description:** - The price of a dozen eggs in the United States from 1900-1993.

```{r warning=FALSE, message=FALSE}
eggs
```

#### Timeseries data plot: pigs

```{r warning=FALSE, message=FALSE}
autoplot(eggs,ylab="Price of a Dozen Eggs",xlab="Year") + ggtitle("Price Trend of a Dozen Eggs in US (1900-1993)")
```

#### Holt's Linear Trend Model Analysis:

##### Model1 - Default Values

```{r warning=FALSE, message=FALSE, fig.width=8, fig.height=8,fig.align="center"}
model1_fit <- holt(eggs, h=100)

model1_fit[['model']]
accuracy(model1_fit)

autoplot(eggs) +
  autolayer(fitted(model1_fit), series='Fitted') +
  autolayer(model1_fit, series="Holt's method", PI=TRUE) +
  ggtitle("Dozen Eggs Price Forecast from Holt's method - Default") + xlab("Year") +
  ylab("Dozen Egg Price") +
  guides(colour=guide_legend(title="Forecast"))

```

##### Model2 - Damped Forecast

```{r warning=FALSE, message=FALSE, fig.width=8, fig.height=8,fig.align="center"}
model2_fit <- holt(eggs, h=100, damped=TRUE, phi = 0.98)

model2_fit[['model']]
accuracy(model2_fit)
autoplot(eggs, coverage.col = "gray50") +
  autolayer(fitted(model2_fit), series='Fitted') +
  autolayer(model2_fit, series="Holt's method", PI=TRUE) +
  ggtitle("Dozen Eggs Price Forecast from Holt's method - Damped Forecast") + xlab("Year") +
  ylab("Dozen Egg Price") +
  guides(colour=guide_legend(title="Forecast"))

```


##### Model3 - With Box Cox Transformation

```{r warning=FALSE, message=FALSE, fig.width=8, fig.height=8,fig.align="center"}

lambda <- BoxCox.lambda(eggs)

model3_fit <-  holt(eggs, h=100, lambda = lambda)

model3_fit[['model']]
accuracy(model3_fit)
autoplot(eggs) +
  autolayer(fitted(model3_fit), series='Fitted') +
  autolayer(model3_fit, series="Holt's method", PI=TRUE) +
  ggtitle("Dozen Eggs Price Forecast from Holt's method - Boxcox Transform") + xlab("Year") +
  ylab("Dozen Egg Price") +
  guides(colour=guide_legend(title="Forecast"))

```

##### Model4 - With Box Cox Transformation and Damped

```{r warning=FALSE, message=FALSE, fig.width=8, fig.height=8,fig.align="center"}

lambda <- BoxCox.lambda(eggs)

model4_fit <-  holt(eggs, h=100, damped=TRUE, phi = 0.98, lambda = lambda)

model4_fit[['model']]
accuracy(model4_fit)
autoplot(eggs) +
  autolayer(fitted(model3_fit), series='Fitted') +
  autolayer(model4_fit, series="Holt's method", PI=TRUE) +
  ggtitle("Dozen Eggs Price Forecast from Holt's method - Boxcox Transform + Damped") + xlab("Year") +
  ylab("Dozen Egg Price") +
  guides(colour=guide_legend(title="Forecast"))

```

#### Model Accuracy Comparisons:

```{r}

df <- rbind(accuracy(model1_fit), accuracy(model2_fit), accuracy(model3_fit), accuracy(model4_fit))
row.names(df) <- c('Default', 'Damped', 'Box-Cox','Damped & Box-Cox')
kable(df) %>% 
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"))
```


#### Observations:

  - I have built 4 models with holt() method with various parameter options.
  - In the 1st model, I have used default parameters and it can be observed that Holt's default method tends to generate forecast with a constand trend over a long forecast period (h=100) which in this case is a decreasing trend. This leads to negative price for dozen eggs around 2025. Clearly this doesn't make much sense.
  - To avoid situation listed above, in the 2nd model I have used **Damped = TRUE** option (phi=0.98). This option made a clear impact in the forecast and it dampend the forecast appropriately.
  - In the 3rd model, I have used BoxCox transformation (with lambda=0.3956) without damped option. In this model the smoothing parameter alpha has higher value than previous 2 models.
  - In the 4th model, I have used BoxCox transformation (with lambda=0.3956) with damped option. In this model the smoothing parameter alpha has highest value amongst the 4 models.
  - Also, 4th model seems to have lowest RMSE and also lowest AIC. So both from minimizing the squared error and maximum likelihood estimation perspective, 4th model seems to perform best.

### Section 7.8 Exercise 8
Recall your retail time series data (from Exercise 3 in Section 2.10).

#### DataSet: Retail

```{r fig.height=10, fig.width=10}
retaildata <- readxl::read_excel("retail.xlsx", skip=1)

head(retaildata, 20) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

```

Select one of the time series as follows (but replace the column name with your own chosen column):

**myts <- ts(retaildata[,"A3349873A"],frequency=12, start=c(1982,4))**

I have selected **"A3349337W"** as the timeseries from the retail data set for this exercise.
  
```{r}
retailts <- ts(retaildata[,"A3349337W"],frequency=12, start=c(1982,4))

retailts

```

```{r fig.height=8,fig.width=12}
title <- 'Retail Sales for Category = A3349337W'

# Timeseries plot before Transformation:
retail_plot <- autoplot(retailts,ylab="$ Sales Turnover",xlab="Year") + ggtitle(title)

retail_plot
```

a) Why is multiplicative seasonality necessary for this series?

#### Multiplicative Seasonality

Based on the timeseries plot for the retail data set, it can be observed that the seasonal variations are not constant throughout the series. From very narrow seasonal spike patterns observed upto 1990, the seasonal variations seems to gradually increase in more recent years. Due to this pattaren in variation of seasonality, multiplicative seasonality is necessary for this dataset.  

b) Apply Holt-Winters' multiplicative method to the data. Experiment with making the trend damped.

#### Holt-Winters' Seasonal multiplicative method

```{r fig.height=8,fig.width=12}
hw_fit <- hw(retailts,seasonal="multiplicative")

# Model Fit Summary
summary(hw_fit)

# Plot
autoplot(retailts) +
  autolayer(hw_fit, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("$Sales Turnover") +
  ggtitle("Holt-Winter's Multiplicative Method: Retail Sales for Category = 'A3349337W'") +
  guides(colour=guide_legend(title="Forecast"))

```

#### Holt-Winters' Seasonal multiplicative method with Damped=TRUE

```{r fig.height=8,fig.width=12}
hw_damped_fit <- hw(retailts,seasonal="multiplicative", damped=TRUE, phi = 0.98)

# Model Fit Summary
summary(hw_damped_fit)

# Plot
autoplot(retailts) +
  autolayer(hw_damped_fit, series="HW multiplicative forecasts",
    PI=FALSE) +
  xlab("Year") +
  ylab("$Sales Turnover") +
  ggtitle("Holt-Winter's Multiplicative Method (w/ Damped Option): Retail Sales for Category = 'A3349337W'") +
  guides(colour=guide_legend(title="Forecast"))

```

Making the trend damped didn't seem to make any major impact on the model.

c) Compare the RMSE of the one-step forecasts from the two methods. Which do you prefer?

#### RMSE comparison of the two models

```{r}
# Accuracy of the Model Fit w/o damping
accuracy(hw_fit)

# Accuracy of the Model Fit w/ damping
accuracy(hw_damped_fit)

```

I prefer the the model without damping and lower RMSE.

d) Check that the residuals from the best method look like white noise.

#### Residual Plot for Best Method:

```{r}
checkresiduals(hw_fit)
```

Based on above output of Ljung-Box test, the p-value is very small. So it cannot be said that residuals are from White Noise.


e) Now find the test set RMSE, while training the model to the end of 2010. Can you beat the seasonal naive approach from Exercise 8 in Section 3.7?

#### Train/Test Split

```{r fig.height=6,fig.width=12}
retail.train <- window(retailts, end=c(2010, 12))
retail.test <- window(retailts, start=c(2011))

autoplot(retailts) +
  autolayer(retail.train, series="Training") +
  autolayer(retail.test, series="Test") +
  ggtitle('Train/Test Split - Retail Data') +
  ylab("$Sales Turnover")

```

#### Model Approach

I have used 3 different methods to fit the training set:

 - Seasonal Naive
 - Holt-Winter's Multiplicative Trend
 - Holt-Winter's Additive Trend, with Box-Cox Transform
 
```{r fig.height=8,fig.width=12}
fit_snaive <- snaive(retail.train, h=36)
fit1_hw <- hw(retail.train, h=36, seasonal='multiplicative', damped=FALSE)

lambda <- BoxCox.lambda(retail.train)
fit2_hw <- hw(retail.train, h=36, seasonal='additive', damped=FALSE, lambda=lambda)

autoplot(retail.test, series='Test Data') +
  autolayer(fit_snaive, series='Seasonal Naive Forecast', PI=F) +
  autolayer(fit1_hw, series="Holt-Winter's Multiplicative Forecast", PI=F) +
  autolayer(fit2_hw, series="Holt-Winter's Additive Forecast + Box Cox", PI=F) +
  guides(colour=guide_legend(title="Legend")) +
  ggtitle('Test Set Forecast') +
  ylab("$Sales Turnover")
 
```
 
#### Test Set RMSE
 
```{r}
df <- data.frame(c('Seasonal Naive Forecast', "Holt-Winter's Multiplicative Method", 
               "Holt-Winter's Additive Method with Box-Cox Transform"), c(accuracy(fit_snaive)[,"RMSE"],accuracy(fit1_hw)[,"RMSE"], accuracy(fit2_hw)[,"RMSE"]))

colnames(df) <- c("Model","RMSE")

df %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="250px")
  
```

From the above table, **Holt-Winter's Multiplicative Method** has the best RMSE beating the snaive model.

### Section 7.8 Exercise 9
For the same retail data, try an STL decomposition applied to the Box-Cox transformed series, followed by ETS on the seasonally adjusted data. How does that compare with your best previous forecasts on the test set?

#### STL Decomposition with Box cox Transform

```{r  fig.height=6,fig.width=12}
lambda <- BoxCox.lambda(retail.train)
paste('Optimal value of lambda for Box-Cox Transformation:', lambda)

stl_bc_fit <- stlf(retail.train, lambda = lambda)

# Model Summary
summary(stl_bc_fit)

autoplot(retail.train, series = "train") +
  autolayer(stl_bc_fit, series = 'STL w/ Box Cox') +
  guides(colour=guide_legend(title="Legend")) +
  ggtitle('STL with Box Cox for Retail Data Set') +
  ylab("$Sales Turnover")


accuracy(stl_bc_fit)
```

#### ETS on Seasonally Adjusted data


```{r fig.height=6,fig.width=12}
ets_retail <- ets(seasadj(decompose(retail.train,"multiplicative")))

# Model Summary
summary(ets_retail)

autoplot(retail.train, series = 'Train Set') + 
  autolayer(forecast(ets_retail, h = 24, PI=F), series = "ETS Forecast") +
  guides(colour=guide_legend(title="Legend")) +
  ggtitle('ETS Forecast for Retail Data Set') +
  ylab("$Sales Turnover")


accuracy(ets_retail)
```

#### RMSE Comparison

```{r}
df1 <- data.frame(c('STL w/ Box Cox Transform', "ETS on Seasonally Adjusted Data"), c(accuracy(stl_bc_fit)[,"RMSE"],accuracy(ets_retail)[,"RMSE"]))

colnames(df1) <- c("Model","RMSE")

df <- rbind(df,df1)


df %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")
```

#### Conclusion

Both the models - STL Decomposition + Box Cox and ETS method on seasonally adjusted data shows lower RMSE than the previous best method.
